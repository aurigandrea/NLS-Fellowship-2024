{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da15a2e",
   "metadata": {},
   "source": [
    "# Link checker\n",
    "The reason behind archiving web content is the transient nature of the internet - web pages are short lived. However, as we can do distant reading only on the metadata and not the archived collection itself, not all the URLs in the dataset are currently live. This link checker helps to select those URLs which are currently active, save them in the DataFrame, ready to scraping more information about the archived websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b84bb",
   "metadata": {},
   "source": [
    "## Filter CSV by Alive URLs\n",
    "Read CSV file with URLs and filter out the rows where the URL is not alive (i.e., returns a 200 OK status).\n",
    "\n",
    "Steps:\n",
    "Read the CSV file into a pandas DataFrame.\n",
    "Define an asynchronous function to check if a URL is alive using the aiohttp library.\n",
    "Apply the function to filter out rows where the URL is not alive.\n",
    "Display or save the filtered DataFrame.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41595f55",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2742f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #This is the dataframe\n",
    "import requests # This is for opening urls\n",
    "import asyncio # These two are for making asynchronous request.\n",
    "import aiohttp\n",
    "from urllib.parse import urlparse #We use it to check the URL fomratting\n",
    "from tqdm.asyncio import tqdm_asyncio #It's for tracking the progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import CVS file to Dataframe\n",
    "Use your own file name. Your csv must have a 'url' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AoT3_without_duplicates_and_xml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50729501",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions to validate the format of your urls and to clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate and clean URLs\n",
    "def clean_and_validate_url(url):\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    url = url.strip()  # Remove leading/trailing whitespace\n",
    "    if not url.startswith('http'):\n",
    "        url = 'http://' + url  # Add http:// if missing\n",
    "    return url if is_valid_url(url) else None\n",
    "\n",
    "# Function to check if URL is valid\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating URL: {url}, Error: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffb318",
   "metadata": {},
   "source": [
    "## Define functions to get the URls and check if they are alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f29b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous function to fetch status of URLs\n",
    "async def fetch_status(session, url):\n",
    "    try:\n",
    "        async with session.head(url) as response:\n",
    "            return url, response.status == 200\n",
    "    except:\n",
    "        return url, False\n",
    "\n",
    "# Asynchronous function to check statuses of multiple URLs\n",
    "async def check_urls(urls):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        with tqdm_asyncio(total=len(urls), desc=\"Checking URLs\", unit=\"url\") as pbar:\n",
    "            for url in urls:\n",
    "                tasks.append(fetch_status(session, url))\n",
    "                await asyncio.sleep(0.1)  # Add a small delay to avoid overloading the server\n",
    "                pbar.update(1)  # Update progress bar\n",
    "            results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Function to run asynchronous URL checking\n",
    "async def is_url_alive_async(urls):\n",
    "    results = await check_urls(urls)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2319b",
   "metadata": {},
   "source": [
    "## Clean and validate URLs in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca266e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's iterative. You check each rows for the urls. If they are clean, all fine, if not, you update them.\n",
    "for index, row in df.iterrows():\n",
    "    original_url = row['url']\n",
    "    cleaned_url = clean_and_validate_url(original_url)\n",
    "    if cleaned_url:\n",
    "        df.at[index, 'url'] = cleaned_url  # Update DataFrame with cleaned URL\n",
    "    else:\n",
    "        print(f\"Invalid URL: {original_url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20057cdb",
   "metadata": {},
   "source": [
    "## Run the link checker\n",
    "It will take a some time depending on the lenght of your url list. The progress bar at the bottom helps to track where you are in the progress. Keep your computer charged, logged in, and your connection alive to avoid disappointment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7635459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of cleaned URLs from DataFrame\n",
    "urls_to_check = df['url'].dropna().tolist()\n",
    "\n",
    "# Run asynchronous URL checking in current event loop\n",
    "results = await is_url_alive_async(urls_to_check)\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['url', 'is_alive'])\n",
    "\n",
    "# Merge results with original DataFrame to retain other columns\n",
    "df = df.merge(results_df, on='url')\n",
    "\n",
    "# Filter DataFrame to keep only rows where the URL is alive\n",
    "alive_urls_df = df[df['is_alive']]\n",
    "\n",
    "# Display first few rows of filtered DataFrame\n",
    "print(alive_urls_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac6eb2",
   "metadata": {},
   "source": [
    "## Export your list of working urls in a new CSV file.\n",
    "This is the base of your final dataset which we are going to enhance and analyse in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "alive_urls_df.to_csv('AoT_good_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf22ad",
   "metadata": {},
   "source": [
    "## Admire your work\n",
    "Display the final dataset containging only the active links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aot]",
   "language": "python",
   "name": "conda-env-aot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
