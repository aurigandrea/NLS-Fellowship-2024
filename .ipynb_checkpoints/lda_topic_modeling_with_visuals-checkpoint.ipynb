{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a606bc8",
   "metadata": {},
   "source": [
    "\n",
    "#  Topic Modeling with LDA (Latent Dirichlet Allocation) and visualisation\n",
    "\n",
    "This notebook applies classical topic modeling using **LDA** from `gensim` on the `full_text` column of one of the enhanced datasets. You can apply it to any textual column - the more text, the merrier. It includes visualisations to explore topics and their distributions.\n",
    "\n",
    "We will:\n",
    "- Clean and preprocess the text\n",
    "- Train an LDA model (5 topics)\n",
    "- Visualise results with word clouds, bar charts, and pyLDAvis\n",
    "\n",
    "The aim of the notebook is having an overview of what the collection contains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fe14d",
   "metadata": {},
   "source": [
    "\n",
    "## 📦 Import Libraries\n",
    "\n",
    "We use:\n",
    "- `pandas` to load the dataset\n",
    "- `nltk` and `spacy` for text preprocessing\n",
    "- `gensim` for topic modeling\n",
    "- `pyLDAvis` and `matplotlib` for visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217ed87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp39-cp39-macosx_10_9_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp39-cp39-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.2-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.1-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wrapt in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.12.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.5-cp39-cp39-macosx_10_9_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp39-cp39-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.12-cp39-cp39-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.2-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.3/443.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.1-cp39-cp39-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp39-cp39-macosx_10_9_x86_64.whl (637 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.3/637.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp39-cp39-macosx_10_9_x86_64.whl (898 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.5/898.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl (21.2 MB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading marisa_trie-1.2.1-cp39-cp39-macosx_10_9_x86_64.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: blis\n",
      "  Building wheel for blis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for blis: filename=blis-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl size=7169822 sha256=85f398c6f96f305e58ed973bec3822356e30a845b0b8e64d71e8a1010108e9e2\n",
      "  Stored in directory: /Users/andreakocsis/Library/Caches/pip/wheels/2b/75/f4/e540f5b1ef1d2610200886c45092f5be21cfe8309ccc48c577\n",
      "Successfully built blis\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, smart-open, shellingham, pygments, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 2.0.2 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 2.0.2 which is incompatible.\n",
      "spyder-kernels 2.1.3 requires jupyter-client<7,>=5.3.4, but you have jupyter-client 7.4.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.0.2 preshed-3.0.9 pydantic-2.11.2 pydantic-core-2.33.1 pygments-2.19.1 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.15.2 typing-inspection-0.4.0 wasabi-1.1.3 weasel-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef668b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyldavis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.7.3)\n",
      "Collecting pandas>=2.0.0 (from pyldavis)\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from pyldavis)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (3.1.6)\n",
      "Requirement already satisfied: numexpr in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (2.8.1)\n",
      "Collecting funcy (from pyldavis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (1.0.2)\n",
      "Requirement already satisfied: gensim in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pyldavis) (61.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyldavis) (2021.3)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyldavis)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=1.0.0->pyldavis) (2.2.0)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy (from pyldavis)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from gensim->pyldavis) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from jinja2->pyldavis) (2.0.1)\n",
      "Requirement already satisfied: packaging in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from numexpr->pyldavis) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim->pyldavis) (1.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from packaging->numexpr->pyldavis) (3.0.4)\n",
      "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: funcy, tzdata, scipy, joblib, pandas, pyldavis\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "Successfully installed funcy-2.0 joblib-1.4.2 pandas-2.2.3 pyldavis-3.4.1 scipy-1.13.1 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyldavis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a805c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (2.0.2)\n",
      "Requirement already satisfied: pillow in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from wordcloud) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Collecting numpy>=1.6.1 (from wordcloud)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp39-cp39-macosx_10_9_x86_64.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, wordcloud\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 wordcloud-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba36b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import nltk\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fe560",
   "metadata": {},
   "source": [
    "## 🔧 Download Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970089db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andreakocsis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea836aaf",
   "metadata": {},
   "source": [
    "\n",
    "## 📄 Load Dataset\n",
    "\n",
    "We load the dataset which contains the parsed information and extract the `full_text` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b338f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1057 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/AoT5_enhanced.csv\")\n",
    "texts = df['full_text'].dropna().astype(str).tolist()\n",
    "print(f\"Loaded {len(texts)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a31ce",
   "metadata": {},
   "source": [
    "\n",
    "## 🧹 Preprocess Text\n",
    "\n",
    "We lowercase, remove punctuation, remove stopwords, and lemmatize the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26a3f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess(texts):\n",
    "    clean_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=500):\n",
    "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n",
    "        clean_texts.append(tokens)\n",
    "    return clean_texts\n",
    "\n",
    "processed_texts = preprocess(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d060a6",
   "metadata": {},
   "source": [
    "\n",
    "## 📚 Create Dictionary and Corpus for Gensim\n",
    "\n",
    "These are the input formats required by `gensim`'s LDA model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f8b7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f7582",
   "metadata": {},
   "source": [
    "\n",
    "## 🤖 Train LDA Model (5 Topics)\n",
    "\n",
    "We use `gensim`'s LDA implementation to discover 5 latent topics in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2c4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=5,\n",
    "                     random_state=42,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ff716",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 View Topics and Keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae76155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"learn\" + 0.023*\"r\" + 0.022*\"n\" + 0.014*\"cookie\" + 0.009*\"u\" + 0.008*\"egg\" + 0.008*\"void\" + 0.008*\"c\" + 0.008*\"skin\" + 0.008*\"register\"')\n",
      "(1, '0.014*\"vaccine\" + 0.008*\"study\" + 0.007*\"covid\" + 0.006*\"report\" + 0.006*\"death\" + 0.006*\"base\" + 0.006*\"datum\" + 0.006*\"use\" + 0.005*\"cancer\" + 0.005*\"risk\"')\n",
      "(2, '0.013*\"health\" + 0.009*\"people\" + 0.007*\"help\" + 0.007*\"use\" + 0.007*\"make\" + 0.006*\"support\" + 0.006*\"long\" + 0.006*\"need\" + 0.006*\"woman\" + 0.006*\"work\"')\n",
      "(3, '0.015*\"people\" + 0.013*\"service\" + 0.012*\"gender\" + 0.012*\"care\" + 0.012*\"healthcare\" + 0.010*\"disability\" + 0.009*\"nhs\" + 0.008*\"support\" + 0.008*\"health\" + 0.006*\"england\"')\n",
      "(4, '0.009*\"say\" + 0.007*\"go\" + 0.007*\"back\" + 0.006*\"pain\" + 0.005*\"police\" + 0.005*\"like\" + 0.005*\"sleep\" + 0.005*\"one\" + 0.005*\"year\" + 0.004*\"everything\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84ca5e",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 Interactive pyLDAvis Visualization\n",
    "\n",
    "This shows topic distances, importance, and term relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb28548",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/Users/andreakocsis/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'SafeFunction' on <module 'joblib._parallel_backends' from '/Users/andreakocsis/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgensim_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:273\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    262\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab[term_ix],\n\u001b[1;32m    263\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_topic_freq\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix],\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency[term_ix],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m'\u001b[39m: log_lift\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    268\u001b[0m                        })\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     ])\n\u001b[0;32m--> 273\u001b[0m top_terms \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_relevance_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_ttd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_lift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_job_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    276\u001b[0m topic_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(topic_top_term_df, \u001b[38;5;28menumerate\u001b[39m(top_terms\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39miterrows(), start_index))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([default_term_info] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(topic_dfs))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParallel\u001b[39;00m(Logger):\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;124;03m''' Helper class for readable parallel mapping.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \n\u001b[1;32m    949\u001b[0m \u001b[38;5;124;03m        Read more in the :ref:`User Guide <parallel>`.\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \n\u001b[1;32m    951\u001b[0m \u001b[38;5;124;03m        Parameters\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;124;03m        ----------\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03m        n_jobs: int, default=None\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;124;03m            The maximum number of concurrently running jobs, such as the number\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m            of Python worker processes when ``backend=\"loky\"`` or the size of\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m            the thread-pool when ``backend=\"threading\"``.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m            This argument is converted to an integer, rounded below for float.\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m            If -1 is given, `joblib` tries to use all CPUs. The number of CPUs\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m            ``n_cpus`` is obtained with :func:`~cpu_count`.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;124;03m            For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. For instance,\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m            using ``n_jobs=-2`` will result in all CPUs but one being used.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03m            This argument can also go above ``n_cpus``, which will cause\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m            oversubscription. In some cases, slight oversubscription can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;124;03m            beneficial, e.g., for tasks with large I/O operations.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03m            If 1 is given, no parallel computing code is used at all, and the\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m            behavior amounts to a simple python `for` loop. This mode is not\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m            compatible with ``timeout``.\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m            None is a marker for 'unset' that will be interpreted as n_jobs=1\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03m            unless the call is performed under a :func:`~parallel_config`\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m            context manager that sets another value for ``n_jobs``.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m            If n_jobs = 0 then a ValueError is raised.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m        backend: str, ParallelBackendBase instance or None, default='loky'\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m            Specify the parallelization backend implementation.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m            Supported backends are:\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \n\u001b[1;32m    976\u001b[0m \u001b[38;5;124;03m            - \"loky\" used by default, can induce some\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;124;03m              communication and memory overhead when exchanging input and\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;124;03m              output data with the worker Python processes. On some rare\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m              systems (such as Pyiodide), the loky backend may not be\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m              available.\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m            - \"multiprocessing\" previous process-based backend based on\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m              `multiprocessing.Pool`. Less robust than `loky`.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;124;03m            - \"threading\" is a very low-overhead backend but it suffers\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;124;03m              from the Python Global Interpreter Lock if the called function\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03m              relies a lot on Python objects. \"threading\" is mostly useful\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m              when the execution bottleneck is a compiled extension that\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;124;03m              explicitly releases the GIL (for instance a Cython loop wrapped\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;124;03m              in a \"with nogil\" block or an expensive call to a library such\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;124;03m              as NumPy).\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m            - finally, you can register backends by calling\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m              :func:`~register_parallel_backend`. This will allow you to\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m              implement a backend of your liking.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \n\u001b[1;32m    994\u001b[0m \u001b[38;5;124;03m            It is not recommended to hard-code the backend name in a call to\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m            :class:`~Parallel` in a library. Instead it is recommended to set\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m            soft hints (prefer) or hard constraints (require) so as to make it\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m            possible for library users to change the backend from the outside\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m            using the :func:`~parallel_config` context manager.\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;124;03m        return_as: str in {'list', 'generator', 'generator_unordered'}, default='list'\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;124;03m            If 'list', calls to this instance will return a list, only when\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;124;03m            all results have been processed and retrieved.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m            If 'generator', it will return a generator that yields the results\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m            as soon as they are available, in the order the tasks have been\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m            submitted with.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m            If 'generator_unordered', the generator will immediately yield\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m            available results independently of the submission order. The output\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m            order is not deterministic in this case because it depends on the\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m            concurrency of the workers.\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m        prefer: str in {'processes', 'threads'} or None, default=None\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m            Soft hint to choose the default backend if no specific backend\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m            was selected with the :func:`~parallel_config` context manager.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m            The default process-based backend is 'loky' and the default\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m            thread-based backend is 'threading'. Ignored if the ``backend``\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03m            parameter is specified.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m        require: 'sharedmem' or None, default=None\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m            Hard constraint to select the backend. If set to 'sharedmem',\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m            the selected backend will be single-host and thread-based even\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m            if the user asked for a non-thread based backend with\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;124;03m            :func:`~joblib.parallel_config`.\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;124;03m        verbose: int, default=0\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m            The verbosity level: if non zero, progress messages are\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m            printed. Above 50, the output is sent to stdout.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03m            The frequency of the messages increases with the verbosity level.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m            If it more than 10, all iterations are reported.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m        timeout: float or None, default=None\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m            Timeout limit for each task to complete.  If any task takes longer\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m            a TimeOutError will be raised. Only applied when n_jobs != 1\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}, default='2*n_jobs'\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m            The number of batches (of tasks) to be pre-dispatched.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m            default and the workers should never starve. Note that only basic\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m            arithmetics are allowed here and no modules can be used in this\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;124;03m            expression.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m        batch_size: int or 'auto', default='auto'\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m            The number of atomic tasks to dispatch at once to each\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m            worker. When individual evaluations are very fast, dispatching\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m            calls to workers can be slower than sequential computation because\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m            of the overhead. Batching fast computations together can mitigate\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m            this.\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m            The ``'auto'`` strategy keeps track of the time it takes for a\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            batch to complete, and dynamically adjusts the batch size to keep\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m            the time on the order of half a second, using a heuristic. The\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m            initial batch size is 1.\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;124;03m            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m            batches of a single task at a time as the threading backend has\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m            very little overhead and using larger batch size has not proved to\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124;03m            bring any gain in that case.\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        temp_folder: str or None, default=None\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m            Folder to be used by the pool for memmapping large arrays\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;124;03m            for sharing memory with worker processes. If None, this will try in\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m            order:\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m              variable,\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m            - /dev/shm if the folder exists and is writable: this is a\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m \u001b[38;5;124;03m              RAM disk filesystem available by default on modern Linux\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m              distributions,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m            - the default system temporary folder that can be\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m              overridden with TMP, TMPDIR or TEMP environment\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m              variables, typically /tmp under Unix operating systems.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m            Only active when ``backend=\"loky\"`` or ``\"multiprocessing\"``.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m        max_nbytes int, str, or None, optional, default='1M'\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m            Threshold on the size of arrays passed to the workers that\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03m            triggers automated memory mapping in temp_folder. Can be an int\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m            Use None to disable memmapping of large arrays.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m            Only active when ``backend=\"loky\"`` or ``\"multiprocessing\"``.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, default='r'\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m            Memmapping mode for numpy arrays passed to workers. None will\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m            disable memmapping, other modes defined in the numpy.memmap doc:\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m            https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m            Also, see 'max_nbytes' parameter documentation for more details.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m        Notes\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;124;03m        -----\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \n\u001b[1;32m   1078\u001b[0m \u001b[38;5;124;03m        This object uses workers to compute in parallel the application of a\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;124;03m        function to many different arguments. The main functionality it brings\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;124;03m        in addition to using the raw multiprocessing or concurrent.futures API\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;124;03m        are (see examples for details):\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m        * More readable code, in particular since it avoids\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m          constructing list of arguments.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m        * Easier debugging:\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03m            - informative tracebacks even when the error happens on\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m              the client side\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03m            - using 'n_jobs=1' enables to turn off parallel computing\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03m              for debugging without changing the codepath\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m            - early capture of pickling errors\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \n\u001b[1;32m   1093\u001b[0m \u001b[38;5;124;03m        * An optional progress meter.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m        * Interruption of multiprocesses jobs with 'Ctrl-C'\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m        * Flexible pickling control for the communication to and from\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m          the worker processes.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m        * Ability to use shared memory efficiently with worker\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m          processes for large numpy-based datastructures.\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m        Note that the intended usage is to run one call at a time. Multiple\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m        calls to the same Parallel object will result in a ``RuntimeError``\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m        Examples\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m        --------\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m        A simple example:\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m        >>> from math import sqrt\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m        >>> from joblib import Parallel, delayed\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m        Reshaping the output when the function has several return\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m        values:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m        >>> from math import modf\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m        >>> from joblib import Parallel, delayed\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m        >>> res, i = zip(*r)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m        >>> res\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m        >>> i\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m        The progress meter: the higher the value of `verbose`, the more\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m        messages:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03m        >>> from time import sleep\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m        >>> from joblib import Parallel, delayed\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m        >>> r = Parallel(n_jobs=2, verbose=10)(\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m        ...     delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m        Traceback example, note how the line of the error is indicated\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;124;03m        as well as the values of the parameter passed to the function that\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m        triggered the exception, even though the traceback happens in the\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03m        child process:\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m        >>> from heapq import nlargest\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m        >>> from joblib import Parallel, delayed\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m        >>> Parallel(n_jobs=2)(\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m        ... delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3))\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m        ... # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m        -----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;124;03m        Sub-process traceback:\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;124;03m        -----------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m        TypeError                                      Mon Nov 12 11:37:46 2012\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m        PID: 12934                                Python 2.7.3: /usr/bin/python\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m        ........................................................................\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m            419         if n >= size:\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m            420             return sorted(iterable, key=key, reverse=True)[:n]\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03m            421\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;124;03m            422     # When key is none, use simpler decoration\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;124;03m            423     if key is None:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;124;03m        --> 424         it = izip(iterable, count(0,-1))           # decorate\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;124;03m            425         result = _nlargest(n, it)\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;124;03m            426         return map(itemgetter(0), result)          # undecorate\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;124;03m            427\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;124;03m            428     # General case, slowest method\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;124;03m         TypeError: izip argument #1 must support iteration\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m        _______________________________________________________________________\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \n\u001b[1;32m   1169\u001b[0m \n\u001b[1;32m   1170\u001b[0m \u001b[38;5;124;03m        Using pre_dispatch in a producer/consumer situation, where the\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03m        data is generated on the fly. Note how the producer is first\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m        called 3 times before the parallel loop is initiated, and then\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m        called to generate new data on the fly:\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m        >>> from math import sqrt\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m        >>> from joblib import Parallel, delayed\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m        >>> def producer():\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m        ...     for i in range(6):\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;124;03m        ...         print('Produced %s' % i)\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;124;03m        ...         yield i\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;124;03m        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;124;03m        ...     delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03m        Produced 0\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m        Produced 1\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m        Produced 2\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m        Produced 3\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m        Produced 4\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m        Produced 5\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1199\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mdefault_parallel_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     ):\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;66;03m# Initiate parent Logger class state\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meffective_n_jobs\u001b[39m(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;124;03m\"\"\"Determine the number of jobs that can actually run in parallel\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    means requesting all available workers for instance matching the number of\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    CPU cores on the worker host(s).\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m    This method should return a guesstimate of the number of workers that can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;124;03m    actually perform work concurrently with the currently enabled default\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    backend. The primary use case is to make it possible for the caller to know\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m    in how many chunks to slice the work.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    In general working on larger data chunks is more efficient (less scheduling\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    overhead and better use of CPU cache prefetching heuristics) as long as all\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    the workers have enough work to do.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m    Warning: this function is experimental and subject to change in a future\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    version of joblib.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.10\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FallbackToBackend(\n\u001b[1;32m    536\u001b[0m         SequentialBackend(nesting_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesting_level))\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m get_memmapping_executor(\n\u001b[1;32m    539\u001b[0m     n_jobs, timeout\u001b[38;5;241m=\u001b[39midle_worker_timeout,\n\u001b[1;32m    540\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_worker_env(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs),\n\u001b[1;32m    541\u001b[0m     context_id\u001b[38;5;241m=\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmemmappingexecutor_args)\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel \u001b[38;5;241m=\u001b[39m parallel\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_jobs\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef44d4",
   "metadata": {},
   "source": [
    "\n",
    "## ☁️ Word Clouds for Each Topic\n",
    "\n",
    "These visualize the most relevant words per topic using `WordCloud`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for topic_id in range(5):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Word Cloud for Topic #{topic_id}\")\n",
    "    word_freqs = dict(lda_model.show_topic(topic_id, 30))\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd440e3a",
   "metadata": {},
   "source": [
    "\n",
    "## 📈 Topic Frequency Bar Chart\n",
    "\n",
    "This chart shows how many documents are most strongly associated with each topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9eef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_counts = defaultdict(int)\n",
    "for doc in corpus:\n",
    "    dominant_topic = sorted(lda_model.get_document_topics(doc), key=lambda x: -x[1])[0][0]\n",
    "    topic_counts[dominant_topic] += 1\n",
    "\n",
    "topics = list(topic_counts.keys())\n",
    "counts = list(topic_counts.values())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(topics, counts, color='skyblue')\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.title(\"Dominant Topic Distribution\")\n",
    "plt.xticks(topics)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
