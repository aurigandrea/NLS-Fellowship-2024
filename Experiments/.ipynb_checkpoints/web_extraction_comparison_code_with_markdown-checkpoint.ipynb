{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad3aa04",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“„ Web extraction comparison: BeautifulSoup/spaCy vs newspaper3k\n",
    "\n",
    "This notebook compares two methods for extracting text, summaries, keywords, and titles from a list of URLs:\n",
    "\n",
    "- **Method 1**: Uses `requests` + `BeautifulSoup` + `spaCy`\n",
    "- **Method 2**: Uses the `newspaper3k` library\n",
    "\n",
    "We evaluate each method by:\n",
    "- Processing a list of URLs\n",
    "- Measuring execution time\n",
    "- Visualizing text lengths\n",
    "- Saving outputs for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df = pd.read_csv('NLS_alive_urls.csv')\n",
    "results_bs_spacy = []\n",
    "results_newspaper = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56832df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_title(soup):\n",
    "    if soup.title:\n",
    "        return soup.title.string.strip()\n",
    "    return \"No title found\"\n",
    "\n",
    "def get_text_and_summary(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text().strip() for para in paragraphs])\n",
    "    if text:\n",
    "        doc = nlp(text)\n",
    "        summary = ' '.join([sent.text for sent in doc.sents][:5])\n",
    "        return text, summary\n",
    "    return \"\", \"No summary available\"\n",
    "\n",
    "def get_full_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    return ' '.join([para.get_text().strip() for para in paragraphs]) or \"No text available\"\n",
    "\n",
    "def get_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop][:10]\n",
    "\n",
    "def process_with_bs_spacy(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title = get_title(soup)\n",
    "            text, summary = get_text_and_summary(soup)\n",
    "            keywords = get_keywords(text)\n",
    "            full_text = get_full_text(soup)\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"keywords\": ', '.join(keywords),\n",
    "                \"full_text\": full_text\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url} with BeautifulSoup/spaCy: {e}\")\n",
    "    return None\n",
    "\n",
    "def process_with_newspaper(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": article.title,\n",
    "            \"summary\": article.summary,\n",
    "            \"keywords\": ', '.join(article.keywords),\n",
    "            \"full_text\": article.text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url} with newspaper3k: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time_bs_spacy = time.time()\n",
    "with tqdm(total=len(df), desc=\"Processing with BeautifulSoup/spaCy\", unit=\"url\") as pbar:\n",
    "    for url in df['url']:\n",
    "        result = process_with_bs_spacy(url)\n",
    "        if result:\n",
    "            results_bs_spacy.append(result)\n",
    "        pbar.update(1)\n",
    "end_time_bs_spacy = time.time()\n",
    "\n",
    "start_time_newspaper = time.time()\n",
    "with tqdm(total=len(df), desc=\"Processing with newspaper3k\", unit=\"url\") as pbar:\n",
    "    for url in df['url']:\n",
    "        result = process_with_newspaper(url)\n",
    "        if result:\n",
    "            results_newspaper.append(result)\n",
    "        pbar.update(1)\n",
    "end_time_newspaper = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c80465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_bs_spacy = pd.DataFrame(results_bs_spacy)\n",
    "df_newspaper = pd.DataFrame(results_newspaper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb725930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Execution time for BeautifulSoup/spaCy: {end_time_bs_spacy - start_time_bs_spacy} seconds\")\n",
    "print(f\"Execution time for newspaper3k: {end_time_newspaper - start_time_newspaper} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e404db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "save_dir = 'plots'\n",
    "ensure_dir(save_dir)\n",
    "\n",
    "def plot_combined_text_length_distribution(df1, df2, method1, method2):\n",
    "    df1['text_length'] = df1['full_text'].apply(lambda x: len(x.split()))\n",
    "    df2['text_length'] = df2['full_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df1['text_length'], kde=True, bins=30, color='blue', label=method1, alpha=0.5)\n",
    "    sns.histplot(df2['text_length'], kde=True, bins=30, color='orange', label=method2, alpha=0.5)\n",
    "    plt.xlabel('Text Length (number of words)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Text Length Distribution Comparison')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'text_length_distribution_comparison.png'))\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_text_length_distribution(df_bs_spacy, df_newspaper, 'BeautifulSoup/spaCy', 'newspaper3k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b05f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_bs_spacy.to_csv('results_bs_spacy.csv', index=False)\n",
    "df_newspaper.to_csv('results_newspaper.csv', index=False)\n",
    "\n",
    "with open('execution_times.txt', 'w') as f:\n",
    "    f.write(f\"Execution time for BeautifulSoup/spaCy: {end_time_bs_spacy - start_time_bs_spacy} seconds\\n\")\n",
    "    f.write(f\"Execution time for newspaper3k: {end_time_newspaper - start_time_newspaper} seconds\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
