{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc720ad",
   "metadata": {},
   "source": [
    "# Part 1: JSON to CSV\n",
    "In this part we begin with observing the structure of the JSON files containging the metadata, and after selecting the information relevant to our inquiry, we export them as a CVS file. It will help us to see it in a column-row structure and helps scraping additional information about the URLs into the dataframe in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559159c",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "We'll start by importing the necessary libraries. You need only json to load the metadata and pandas to convert it to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efeef82",
   "metadata": {},
   "source": [
    "## Load the JSON Data\n",
    "We'll load the JSON data from a the metadata file. Replace .json with your actual file name if you want to use an other dataset. We print the fist entries to have a glimpse into the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from a file\n",
    "with open('AoT_metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data to understand the structure\n",
    "print(json.dumps(data, indent=2)[:1000])  # Print the first 1000 characters to avoid too much output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2e52b",
   "metadata": {},
   "source": [
    "## Extract Required Fields and Create a DataFrame\n",
    "From the structure we see that we need only id, url, and description values to work with the dataset. We'll extract the id, url, and description from each entry and create a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise lists to store IDs, URLs, and descriptions\n",
    "ids = []\n",
    "urls = []\n",
    "descriptions = []\n",
    "\n",
    "def extract_data(entry):\n",
    "    # Recursively extract data from nested dictionaries\n",
    "    if isinstance(entry, dict):\n",
    "        if 'id' in entry and isinstance(entry['id'], (int, str)) and 'urls' in entry and isinstance(entry['urls'], list) and 'description' in entry and isinstance(entry['description'], str):\n",
    "            for url in entry['urls']:\n",
    "                ids.append(entry['id'])\n",
    "                urls.append(url)\n",
    "                descriptions.append(entry['description'])\n",
    "        else:\n",
    "            for key, value in entry.items():\n",
    "                extract_data(value)\n",
    "    elif isinstance(entry, list):\n",
    "        for item in entry:\n",
    "            extract_data(item)\n",
    "\n",
    "# `Data` is your nested JSON data structure\n",
    "for entry in data.values():  # Use .values() if data is a dictionary\n",
    "    extract_data(entry)\n",
    "\n",
    "# Create a DataFrame with the extracted data\n",
    "df = pd.DataFrame({'id': ids, 'url': urls, 'description': descriptions})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfad60a",
   "metadata": {},
   "source": [
    "## Save Extracted Data to a New File\n",
    "You can save now the DataFrame with the extracted data to a new CSV file called AoTfull_extracted.csv. Replace with the name you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('AoTfull_extracted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63453b",
   "metadata": {},
   "source": [
    "## See how many entries you got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25ddaf",
   "metadata": {},
   "source": [
    "# Part 2: Dropping duplicates and unnecessarily links\n",
    "Sometimes there are duplicates in the dataset and many extensions are not sitable for scraping and parsing (such as XML), but are collected in web archives. Therefore we aim to keep only the links that are useful for later analysis and are not redundant. This example demostrates dropping xml, but you can adjust it to drop .cvs, .pdf ending links, social media links (with the help of REGEX) or simply to keep only .html links in the 'url' column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544812f2",
   "metadata": {},
   "source": [
    "## Look at the duplicates before dropping them - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the DataFrame by 'url' column to group duplicates together\n",
    "\n",
    "df.sort_values(by='url', inplace=True)\n",
    "\n",
    "#Identify duplicates based on 'url' column\n",
    "duplicates = df[df.duplicated(subset=['url'], keep=False)]\n",
    "\n",
    "# Display duplicates to inspect before dropping\n",
    "print(\"Duplicate rows based on 'url' column:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf50789",
   "metadata": {},
   "source": [
    "## Use drop_duplicates to keep only the first occurrence of each unique 'url'.\n",
    "We also print the shape of the dataset to see how many rows we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the cleaned DataFrame to verify the results using head().\n",
    "We have a glimpse into the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder DataFrame to original index\n",
    "df.sort_values(by='id', inplace=True)\n",
    "\n",
    " # Displaying the first few rows to verify the cleaning process\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27fba4",
   "metadata": {},
   "source": [
    "## Save the cleaned DataFrame (df) to a new CSV file named using to_csv(). \n",
    "We export the final csv, ready for the link checker in the next stage. The parameter index=False ensures that the index column is not saved to the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0505fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('AoTfull_without_duplicates_and_xml.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aot]",
   "language": "python",
   "name": "conda-env-aot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
